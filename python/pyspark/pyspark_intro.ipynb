{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apache Spark is a preferred quick, real-time framework using to manipulate big data. \n",
    "# The base language used is Scala, but some people prefer to use Python --> PySpark\n",
    "# Hadoop performs the batched data processing while spark can stream, compute, and also batch process. \n",
    "# Spark uses HDFS (Hadoop distribute file system) for storing data and runs apps on YARN too. \n",
    "# PySpark offers PySpark Shell which allows us to integrate python libraries with the spark framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext is the entryway to spark. It uses the library P44J to launch a JVM. \n",
    "# [https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm] for more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important concept in Spark - RDD (Resilient Distributed Dataset)\n",
    "# --> the elements that run on multiple nodes to do parallel processing on a data cluster.\n",
    "# (what makes working with big data so much faster than it would be)\n",
    "# RDDs are immutable and fault tolerant, you can transform them and act on them. \n",
    "\n",
    "# Transformation: operations applied to an RDD to make a new one. ex. Filter, map\n",
    "# Action: operations appied on RDD that compute something and send response back\n",
    "\n",
    "# To apply any operation in Spark, must create a PySpark RDD first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark RDD class\n",
    "\n",
    "class pyspark.RDD (\n",
    "   jrdd, \n",
    "   ctx, \n",
    "   jrdd_deserializer = AutoBatchedSerializer(PickleSerializer())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run basic operations with pyspark\n",
    "\n",
    "words = sc.parallelize ( # this is an RDD\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"count app\")\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "counts = words.count()\n",
    "print \"Number of elements in RDD -> %i\" % (counts)\n",
    "\n",
    "# cmd: $SPARK_HOME/bin/spark-submit count.py\n",
    "# output: Number of elements in RDD â†’ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"ForEach app\")\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "def f(x): print(x)\n",
    "fore = words.foreach(f) \n",
    "\n",
    "# cmd: $SPARK_HOME/bin/spark-submit foreach.py\n",
    "\n",
    "# output:\n",
    "# scala\n",
    "# java\n",
    "# hadoop\n",
    "# spark\n",
    "# akka\n",
    "# spark vs hadoop\n",
    "# pyspark\n",
    "# pyspark and spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "# map \n",
    "# reduce\n",
    "# join "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
