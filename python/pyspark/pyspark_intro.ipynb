{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apache Spark is a preferred quick, real-time framework using to manipulate big data. \n",
    "# The base language used is Scala, but some people prefer to use Python --> PySpark\n",
    "# Hadoop performs the batched data processing while spark can stream, compute, and also batch process. \n",
    "# Spark uses HDFS (Hadoop distribute file system) for storing data and runs apps on YARN too. \n",
    "# PySpark offers PySpark Shell which allows us to integrate python libraries with the spark framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext is the entryway to spark. It uses the library P44J to launch a JVM. \n",
    "# [https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm] for more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important concept in Spark - RDD (Resilient Distributed Dataset)\n",
    "# --> the elements that run on multiple nodes to do parallel processing on a data cluster.\n",
    "# (what makes working with big data so much faster than it would be)\n",
    "# RDDs are immutable and fault tolerant, you can transform them and act on them. \n",
    "\n",
    "# Transformation: operations applied to an RDD to make a new one. ex. Filter, map\n",
    "# Action: operations appied on RDD that compute something and send response back\n",
    "\n",
    "# To apply any operation in Spark, must create a PySpark RDD first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[https://www.tutorialspoint.com/pyspark/pyspark_rdd.htm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark RDD class\n",
    "\n",
    "class pyspark.RDD (\n",
    "   jrdd, \n",
    "   ctx, \n",
    "   jrdd_deserializer = AutoBatchedSerializer(PickleSerializer())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run basic operations with pyspark\n",
    "\n",
    "words = sc.parallelize ( # this is an RDD\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"count app\")\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "counts = words.count()\n",
    "print \"Number of elements in RDD -> %i\" % (counts)\n",
    "\n",
    "# cmd: $SPARK_HOME/bin/spark-submit count.py\n",
    "# output: Number of elements in RDD â†’ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"ForEach app\")\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "def f(x): print(x)\n",
    "fore = words.foreach(f) \n",
    "\n",
    "# cmd: $SPARK_HOME/bin/spark-submit foreach.py\n",
    "\n",
    "# output:\n",
    "# scala\n",
    "# java\n",
    "# hadoop\n",
    "# spark\n",
    "# akka\n",
    "# spark vs hadoop\n",
    "# pyspark\n",
    "# pyspark and spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"Filter app\")\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "words_filter = words.filter(lambda x: 'spark' in x)\n",
    "filtered = words_filter.collect()\n",
    "print \"Fitered RDD -> %s\" % (filtered)\n",
    "\n",
    "# cmd: $SPARK_HOME/bin/spark-submit filter.py\n",
    "# output: \n",
    "Fitered RDD -> [\n",
    "   'spark', \n",
    "   'spark vs hadoop', \n",
    "   'pyspark', \n",
    "   'pyspark and spark'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map \n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"Map app\")\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "words_map = words.map(lambda x: (x, 1))\n",
    "mapping = words_map.collect()\n",
    "print \"Key value pair -> %s\" % (mapping)\n",
    "\n",
    "# cmd: $SPARK_HOME/bin/spark-submit map.py\n",
    "# output:\n",
    "Key value pair -> [\n",
    "   ('scala', 1), \n",
    "   ('java', 1), \n",
    "   ('hadoop', 1), \n",
    "   ('spark', 1), \n",
    "   ('akka', 1), \n",
    "   ('spark vs hadoop', 1), \n",
    "   ('pyspark', 1), \n",
    "   ('pyspark and spark', 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "sc = SparkContext(\"local\", \"Reduce app\")\n",
    "nums = sc.parallelize([1, 2, 3, 4, 5])\n",
    "adding = nums.reduce(add)\n",
    "print \"Adding all the elements -> %i\" % (adding)\n",
    "\n",
    "# cmd: $SPARK_HOME/bin/spark-submit reduce.py\n",
    "# output: Adding all the elements -> 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"Join app\")\n",
    "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
    "y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)])\n",
    "joined = x.join(y)\n",
    "final = joined.collect()\n",
    "print \"Join RDD -> %s\" % (final)\n",
    "\n",
    "# cmd: $SPARK_HOME/bin/spark-submit join.py\n",
    "# output:\n",
    "Join RDD -> [\n",
    "   ('spark', (1, 2)),  \n",
    "   ('hadoop', (4, 5))\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
